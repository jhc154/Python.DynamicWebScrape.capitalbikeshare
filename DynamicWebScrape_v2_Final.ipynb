{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from zipfile import ZipFile\n",
    "from io import BytesIO\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "#This script will get all zipped csv files from CapBikeShare and combine them into a single data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define 'sourceURL' as the target page that contains the list of zipped csv files\n",
    "sourceURL = \"https://s3.amazonaws.com/capitalbikeshare-data/index.html\"\n",
    "\n",
    "#call chrome webdriver from a local source as 'driver'\n",
    "#requires chrome driver from https://sites.google.com/a/chromium.org/chromedriver/downloads\n",
    "driver = webdriver.Chrome('/Users/cheese/lData/chromedriver')  #Must change path to location of your chrome driver\n",
    "\n",
    "#use webdriver to call the URL and define it as 'localPage' \n",
    "localPage = driver.get(sourceURL)\n",
    "\n",
    "#This cell results in launching target URL in a browser controlled by automated test software"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pass the page from Selenium to BeautifulSoup to begin webscrape\n",
    "page_source = driver.page_source\n",
    "soup = BeautifulSoup(page_source, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get and store all URLs into an array as 'links'\n",
    "links = []\n",
    "\n",
    "for link in soup.findAll('a', attrs={'href': re.compile(\"https://\")}):\n",
    "        links.append(link.get('href'))\n",
    "        \n",
    "#confirm by viewing all stored URLs in the 'link' array\n",
    "print(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame() #create a new dataframe as 'df' that will contain all rows from all csv extractions\n",
    "\n",
    "for fileURL in links: #iterate over each stored URL in the 'link' array\n",
    "    if fileURL.endswith('.zip'): #Only process URLs ending in .zip to skip scraped links such as index.html\n",
    "        content = requests.get(fileURL) #use requests.get to store each URL's content as 'content'\n",
    "        zf = ZipFile(BytesIO(content.content)) #use ZipFile to open and read zip data directly with BytesIO\n",
    "    #Use the zip file name to identify only the csv files for extraction that match the zip file name   \n",
    "        for item in zf.namelist():\n",
    "            print(\"File in zip: \"+ item)\n",
    "        fileMatch = [s for s in zf.namelist() if \".csv\" in s][0]\n",
    "        #for the current iteration where the csv file name matches, extract csv from zip, and read into a dataframe 'df_n'\n",
    "        df_n = pd.read_csv(zf.open(fileMatch), low_memory=False) #df_n will be overwritten with each iteration\n",
    "        df = df.append(df_n) #Append each new iteration's dataframe from 'df_n' to 'df'\n",
    "#all file names should display in the order that they are processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the dataframe 'df' that contains all data from Cap Bike Share\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
